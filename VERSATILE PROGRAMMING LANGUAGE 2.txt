VERSATILE PROGRAMMING LANGUAGE 2

Key principles:
- Uninitialized data (variables/arrays on stack, on heap, in registers, in comms buffers/queues, anywhere) must not be allowed UNLESS explicitly asked for (e.g. as an optimization when the programmer knows it'll definitely get overwritten before use - this is often very important in time-critical use cases).
- Several (conceptually) atomic initializations must be supported : default-construction (default ctor of the type runs), zero-value-construction (in case it differes from the default construction), and user-specified construction (via constructor or external compatible init-procedure). The last option must be leaving the data uninitialized, see above.
- Initialization must be protected from concurrent access (critical section or other means). Static analysis + runtime check/locking!
- Concurrency and parallel processing must be part of both the language and the standard library.
- Concurrency and parallelism must aim at correctness and convenience first and only then try to squeeze out the last 10% of performance.
- Concurrency and parallelism must encourage and/or require algorithm construction which is safe or at least not very unsafe. Therefore, low-level, overly manual, unchecked (neither statically nor at runtime) constructs must be left out of the core language and only accessible (if at all!) via extern sections / unsafe sections. Higher-level, human-traceable parallel constructs with useful support from the compiler and runtime can do the job in most cases. Human time waste and buggy operation are way more expensive than a few % of unused/unoptimized CPU time and/or a few KB or MB of RAM.
- The language and runtime must prefer composability at various levels and in many places. Things which were not designed to compose should be given less prominence and discouraged (e.g. some ways of doing inheritance-based OOP, threading vs tasking, and others).
- The language must prefer regularity unless good reasons exist not to for a particular feature.
- The language must have several profiles of clearly defined purpose and availability of language constructs and runtime facilities in each. Basically, these are a subset of (System size: Tiny|Small|Medium|Large) X (Realtime requirements: Hard|Soft|None) :
-- Tiny Hard-real-time Systems Profile (microcontrollers with ~1-2 KB RAM, 10s of KB to 100s of KB of flash storage, 8-bit - e.g. AVR8.) with only the most essential features + permission to use a few low-level constructs which are otherwise banned in high profiles. Very Simple Tasking!
-- Small Hard-real-time Systems Profile (16-bit but mostly 32-bit microcontrollers, 100s of KB of RAM to 10s of MB, several MB to 10s of MB of flash storage, possibly and RTOS). More language constructs and runtime facilities, but still deterministic execution. OS interfacing. Simple Tasking.
-- Small Soft-realtime Systems Profile (e.g. mobile phones and some non-critical control systems which control processes which are relatively slow by nature).
-- Mid-size Application Systems Profile (essentially stuff like tablets and ordinary apps on powerful smartphones). No big deal if something isn't realtime but simply fast enough - which means a lot of language features and runtime support become available and even mandatory. Tasking is not restricted to simple predictable subsets.
-- Mid-size Soft-realtime Systems Profile (Some industrial monitoring and dashboard applications, many multimedia applications, some small internet or LAN servers, many OS components (kernelspace or userspace)).
-- Mid-size Hard-Realtime Systems Profile (Typical industrial control and monitoring applications, some middleware servers, many OS components (kernelspace or userspace - perhaps entire micro/hybrid kernels).
-- Large Application Systems Profile (Desktop-class machines, laptops, servers).
-- Large Soft-realtime Systems Profile (Servers for some time-critical purposes, some industrial monitoring and dashboard applications, time-critical complex midleware, time-critical distributed systems, OS kernels of monolithic/hybrid architecture etc.)
-- Large Hard-realtime Systems Profile (OS kernels of monolithic/hybrid architecture, highly time-critical complex middleware, highly time-critical distributed systems).

- The language and runtime must encourage and require clear, explicit specification of intentions; where defaults and automation are used, they must follow the principle of least surprise. This doesn't mean extreme verbosity of code, but definitely means NO terse and cryptic code. Searchability of the source code and traceability (not only by compiler but also by humans) of things must be a priority, too. In this respect:
-- The type system must forbid implicit conversions even if they are non-narrowing, even inside arithmetic expressions. Some conversions may be needed when dealing with enumerations, constants, booleans and conditionals, and references, but in general C-like or C++-like conversions are not to be performed implicitly.
-- The type system must forbid arbitrary casting. Const-ness, volatility, atomicity, and the undecorated type itself must be respected even across code coming from different places and authors; casting in the sense of reinterpreting is to be permitted only in a few cases where it is truly needed. Violating types out of convenience or for minor performance gains is not good for reliability, safety, and architecture.
-- The type system must  always make a clear distinction between conversion (e.g. int->double or the other way round) and reinterpretation (kind of like C++ reinterpret_cast<>). Conversion modifiers must be provided for some conversions, mostly those involving numerical types, to control how things like overflows and roundoff are handled. Leaving everything to be implementation defined or undefined isn't useful nor safe! When the hardware can't provide good enough guarantees, strict beahviour must be emulated via libraries or inline sequences of instructions (like Java strict FP, Boost lexical_cast<>, etc.).
-- The language must not be a slippery slope of value, reference, parameter-passing conventions, etc. It must always be absolutely clear what a variable or constant is and how it is being treated. Java isn't doing it right, C++ isn't at all (it confuses and intertwines things which should be set apart : values, lvalue and rvalue references, pointers), and functional languages usually overcook things. The language must be designed to allow for extracting reasonable performance without resorting to tricks, and must be intuitively programmable.
-- The language must pay due attention to the fundamental differences between pure value types, pure references, and reference-containing types.
-- The language must forbid unrestricted pointer arithmetic, prevent random garbage in reference, and proactively fight dangling references (make them impossible in many cases and expose them otherwise). Of course, low-level code does reguire some freedom, so limited forms of potentially dangerous operations must be allowed, but not everywhere, and not in all application profiles. Unsafe sections, annotations, etc.
-- The language must forbid keeping and creating direct references to parts of objects which are not independently-existing, proper, subobjects. E.g. descriptors, accessors, iterators, slices, and so on must provide sub-object access services. Those must always keep a reference to the entire object, and use other means to refer to its parts.
-- The above 4-5 requirements make garbage collection and compile-time proofs and optimizations feasible and efficient.
-- The language must provide a rich set of reference types, suitable for different purposes and circumstances. The language and runtime must provide support for limited manual reference management, for both synchronous and asynchronous finalization/destruction, and for good management and reclamation of non-memory resources. Different application profiles must support different resource management modes and techniques. In general, tiny devices can't be treated the same as large applications, and different approaches to resource management are suitable for hard-, soft-, and non-realtime applications. For some of them highly determinstic refcounting is the logical choice, for others tracing collectors and prgramming simplicity are the best; some need to take care of cyclic structures and some don't, and so on. It's not at all good or necessary to expect to provide a single and equally apt interface to resource acquisition and disposal for all application profiles. So yes, some code will need to have specialized versions.
-- The language must have a type system which is flexible, understandable, and layered. Not every application profile has to or should or physically can support all of the layers.
--- The types must be divided into 3 general categories :
---- Pure value types : these contain no references and are trivially memory-copiable.
---- References : these are references to objects; there are several flavours; specific compile-time and/or run-time care is required to handle them, depending on the flavour.
---- Reference-containing types : these are the most complicated in terms of handling.
--- Arrays != references; they don't decay to pointers; they are objects which always know their size. Whether and where the size is stored at runtime depends on the flavour. Access via indexing, iterators, slices, etc - but not pointer arithmetic.
--- Errors != exceptions. Exceptions are exceptional events -- assume any of them can happen any time. When you really expect it, it's a good old error and not a true exception, no matter whether represented by a return code, errno, or something thrown (up). Checking for errors is what should be enforced rather than for truly exceptional events. "How can you expect it if you have no idea it is to be expected?"
--- Functions != procedures. Sorry, but C-family languages (including Java) mixed them up. Function <=> take read-only parameters and compute a result and error status without changing any state; procedure <=> take all sorts of parameters of which usually there's at least one writable (but not always!), do something to the state of those writable parameters and/or to the program and/or system state (e.g. system calls to OS), and return an error status but not anything else. Any subroutine call returns an error code, always (if not specified by programmer, defaults to SUCCESS). Error status != boolean value, probably an integer or integer-based enumeration (or references? That's the most flexible thing! Of course, it must be light on performance - a NULL reference probably is good enough. Like in the Go language "err".)
--- Parameters to functions : always Read-only (IN); parameters to procedures : IN, OUT (write-only, no reading allowed), INOUT (read-write). Parameters quite close to C++ lvalue references (and rvalue ones sometimes). ??/ by reference or by value? explict copy/deepcopy/?. OUT and INOUT always passed by (internal) reference (object has to be modified); IN passed by value if a pure value type (primitive numerical/logical/bool type, char type, or struct type) and by reference if a reference-containing type???
--- Only concurrency-enabled references are allowed to cross concurrent task boundaries; only concurrency-enabled objects (wrapped or otherwise) are allowed to be pointed to by concurrency references.
--- Concurrency : Favor higher-level constructs where possible, especially at larger application profiles. Reasoning about and checking real-time properties is also much more feasible with structured higher-level constructs.
---- Container-based data parallelism: parallel for loops, with manual or automatic partitioning. Several flavours:
----- 
---- Task-based parallelism: several flavours :
---- Only objects which are marked for concurrent use may be used in concurrent contexts; read-only (const or immutable) objects are also OK, with some precautions. Objects marked as concurrent are given locks or atomic instruction treatment by the compiler and runtime. No need to do manual locking. Recursive locks most of the time + lock elision. Reader-writer locks specifiable. Both modifying and non-modifying function/method calls and operations require synchronization to ensure consistent state is written and consistent state is read.
---- "Const" is for constants (strictly compile-time computable stuff! Similar to C++11 "constexpr"); "immutable" is a superset of const which includes stuff initialized at run-time but not modifiable once fully initialized.
---- Const stuff is concurrent-safe. Immutable stuff is safe, too, but only after initialization is complete. This is enforced.
---- Objects of any type can be declared const or immutable.
---- Entire types can include immutable qualifiers. Any object of such a type is immutable, exactly as if explicitly declared as immutable.
---- Immutable objects require no locking or atomics once fully constructed.
---- Strings are, by default, immutable.
---- A third concept exists : that of frozen objects, which can be made read-only on a temporary basis and then reverted to writable state. (Un)Freezing is done with "freeze"/"unfreeze" keywords. Both are idempotent operations. Freezing/unfreezing can succeed only on objects which are not part of multiple access contexts at the moment (comple time error or runtime error if not so). This basically means that if an object is potentially accessible from more than 1 place (variable, concurrent task, etc.), it cannot be frozen (since not all clients will know about that). Read-only access is illegal after a transition frozen->unfrozen; ??? Freeze queues???
---- Types can be qualified as "noconcurrent" to forbid being used in concurrent contexts.
---- Functions, the way they are defined, don't have any state (no static variables allowed!). Use procedures wrapping functions instead.
---- Types can use type-wide static storage/state; for types marked for concurrent use, locking/atomics are automatically applied to it. Types not marked for concurrent use which contain type-wide static fields are forbidden in concurrent contexts, as even if the access to an object is synchronized, there may be many different objects of the type which simultaneously access the type-wide state.
---- For types which contain no references (i.e. pure value types), it is sufficient to lock the object. For references themselves and reference-containing types, recursive traversal and locking along the reference graph is needed in the general case. Circular refs need to be handled as well. It can all get heavy and surprising (e.g. hard-to-spot deadlocks). For these reasons, reference-containing types are treated the following way: if the type or variable is custom_concurrent, then its custom_concurrent_graph_lock() method is called, asssuming the programmer(s) will do the right thing further on. Else, it the object is locked itself and the referred to objects are looked at : if they are all custom_concurrent, they are all locked in one go (i.e. atomically),  ???
---- Global storage/state which is mutable is automatically wrapped in locks. Every access to such objects uses the locks implicitly. The locks are recursive. Reader-writer specifiable. Actually, global state is always contained within a special "Global" module.
--- Per-Module storage/state : static storage visible only to entities within a module; concurrency protected. If the compiler can prove that no concurrent access can ever happen in the program, locks should be omitted (save writeable data section space) and elided (save locking/unlocking overhead).
--- When specilized concurrency-safe data structures and algorithms are available, for example queues using atomics, the compiler can be told that it is safe to not provide the default concurrency protection. A type, free-standing function or procedure, or a variable can be marked "custom_concurrent". No locking etc. is auto-generated then, but any arguments in calls to methods/functions/procedures are required to be of either "concurrent" or "custom_concurrent" qualification. A "custom_concurrent" variable may, for example, hold a frequently updated counter which is allowed to have missed updates due to non-atomic concurrent increment operations.
---- Static immutable storage/state needs no concurrency protection.
---- The programmer can specify a bunch of independent calls as executable cocnurrently/in parallel. There are several flavours:
----- "auto_concurrent_dynamic" : The compiler and runtime are jointly given the responsibility to decide if the calls can indeed be executed concurrently. The compiler prints out a notification to the programmer.
----- "auto_concurrent_static" : The compiler performs the decision only based on static analysis. It prints out a notification to the programmer.
----- "force_concurrent" : The programmer makes the sole decision, manually.
----- To specify the desired level of parallelism : "parallel=<NumOfThreads>", default value is "=auto", i.e. the runtime decides.
At runtime, the calls which are independent are wrapped into tasks which are put into the same task-of-tasks; the one(s) which are found to depend on other task(s) are put into another task-of-tasks, and the execution dependency graph is adjusted accordingly.

--- Tasking, threading, etc. : Concurrency is based on tasking which is based on several things:
---- Execution dependency graph : one per Execution domain.
---- Execution domain : now that's interesting. Execution domains are somewhat like OS processes (and can be implemented as such) : there is a certain degree of memory/resource isolation. Namely: An execution domain does not implicitly share memory with other execution domains; various levels of decsriptor sharing can be specified; all communication occurs via some (virtual) IPC mechanism. The language and runtime provide virtual communication channels/message queues for this + a control API. Commands are sometimes also queued, to ensure e.g. safe destruction of the context inside the domain. Forced quit also exists. Flow of control is based on tasking primitives and messaging - there is no direct calling; error status and computation results are returned via message-based task packaging mechanisms, and exceptions/critical signals are injected as exceptions into the context of the task on whose behalf other tasks are being executed. Non-critical signals are wrapped into messages and queued. Execution domains may or may not have various resource limits, and, depending on the platform, may or may not support various scheduling modes (preemtive, cooperative, etc.).
---- Tasks which are very closely connected (share memory implicitly and more) are only executable in the same execution domain. Every task which is not so tightly coupled with other tasks can be launched into any execution domain, subject to various additional constraints.
---- Every application starts with one primary execution domain, with 2 default tasks : the primaryED._init_main task and the primaryED._main_main task (the latter depends on the former, of course).
---- Tasks can be of several types, according to logical role and privileges:
----- The _main_main task of the primary ED has total control over every other task in the ED and over all secondary EDs and the individual tasks in them.
----- The program ends (logically) when the _main_main_ task of the primary ED ends (via return or (propagated) exception). The program ends (physically, from the OS point of view) when the _main_finalization task of the primary ED ends (via return or locally-encountered fatal exception).
----- Each single ED has similar tasks, with similar purposes. Secondary domains may have automatically-generated default intit, setup, and finalization tasks, or may be launched with user-supplied custom ones. The default ones basically serve as slave servers to the primary ED.
----- The main tasks of each secondary domain are allowed to exert total control over tasks within that ED.

-- Classical tons-of-locks approach is not considered for usability reasons. Instead, configurable Software Tranactional Memory is used, along with message-passing-oriented + strict ownership.
--- Concurrent access correctness issues are actually ownership issues. When something that doesn't really (explictly) own an object or entire object (sub)graph tries to access it behind the owner's (or multiple owners'!) back(s), bad stuff happens. If the owner is one, and if it knows it is the sole owner AND everyone else knows that too, the best approach is "ask it to do something on your behalf and let it manage if, how, when" -- i.e., send a request but don't touch the owned thing yourself. If the service can't be performed, a reply can say it can't, provide reasons, etc. Queueing may be used, which may actually lead to high efficiency due to batching optimization opportunities. It is still possible to have entity A ask B for something, which B can't provide before A has provided B with something else, so deadlocks are still possible (and indirect/trnsitive ones too). But, with a message-oriented approach, the parties are not so closely tied, so they may be much more adaptive. Once a reply message is received saying "can't satisfy request right now", the requester can back off for some time, or retry immediately, retry at most N times, or keep retrying indefinitely - so, deadlock doesn't happen as nothing is actually being locked - what can happen is at most livelock. Some sort of management/scheduling engine can be used to mitigate such problems by arbitration, ordering, etc.
--- Using shared ownership (e.g. via some C++-like shared_ptr which does refcounting, or via non-refcounting shared reference) and concurrent access is exactly a violation of the single ownership principle. If a thing isn't visible from more than 1 place, there's no confusion of formal ownership, effective ownership, ownership duration, etc. Unique_ptr style is actually much better for safe sharing, as it enforces single ownership. For example, if a task A needs to spawn off another task B to process (modify!) some data A owns, the following should happen: 1. A transfers ownership to B; 2. A sleeps or does something else while letting B run to completion; 3. B transfers ownership back to A before death; 4. A now has the exclusive ownership again and so may do whatever it wants with the object. If at any time A or B attempts any (even read-only) access to the data while it's not currently in "legal possession" of it, a fatal runtime error will occur. This approach requires only atomic transfer of ownership (one or 2 pointer-sized locations)(+perhaps a barrier), and absolutely no locking or atomics for actually accessing the data once ownership transfer is complete. Finer-grained, highly-concurrent access to e.g. writeable arrays can be provided by creating non-overlapping accessor objects such as slices, each of which provides unique ownership of part of the underlying array. For linked (non-contiguous) data structures, things will often be more complex, but the approach of asking the container/data structure to provide safe partial accessors and oversee the access is still valid.
For more complex or simply more general access patterns, a gather/scatter approach is applicable, i.e. something which involves what is essentially transactions. 2-Phase Commit and so on.
Gather:
For every R in Resources
	send message to R that we want it to quiesce its state;
	if reply is OK then gather Value(R) and proceed with next R else cancel all reservations and free Values so far and retry loop;
If all reservations are OK, we have gathered all data, then proceed working on the copies...
When (and if) time comes to write anything back, commit or abort...

Actually, there can be several scenarios:
1. Only reads are in the transaction : this means all reservations can be released after the gather stage.
2. Only writes -||- : this means there's only a scatter stage.
3. A mix of reads and writes -||- : this means both stages are needed.

If there is long-running processing between gather and scatter in 3., support should be provided for another transaction preempting the long-running one. Read ops of the original data will not cause abort, but write will (as data will have changed in the meantime).
Transaction ordering and prioritization may very much alleviate non-determinism concerns.

When CPUs become able to lock multiple cache lines, it will all be so much easier... Haswell coming but not so soon...

If type or var not marked as concurrent : don't allow use in concurrent context (i.e. passing into a Task-related procedure).
If type or var is concurrent-enabled/wrapped, then all operations inside it are by default subject to transaction control??

Transferred variables : OK.
What about those which can't and so haven't been transferred but need to be accessed from time to time, like e.g. some I/O object? Transfer is good for task data handoff, but not for dynamic situations which are common in GUIs, servers, other interactive apps?
A "borrow" mechanism is needed then. Temporary transfer of ownership (access), which causes the owner-in-principle to be unable to use the variable. If all concurrent vars are implemented as references, then a single atomic CAS or even atomic INC/DEC will be enough to safely and quickly transfer ownership. But what should the owner-in-principle do? It must assume that the reference may be borrowed at any time (which is closer ot being stolen!) and then perform checks and busy-wait or something else. Gets bad quickly. Asynchronous ownership transfer is hard to use as most async things are. Plus, it's kind of like it's not clear who's the owner of the object in question -- which is a violation of the single owner principle!
So, better have a clear single owner of concurrently-accessible mutable variables : the transaction/concurrent access manager!
Declared a type or object as concurrent => it's owned by the TM/CA; you don't get the actual object but a handle to it. Something like a weak reference : correct, as you don't own it!
Then, whenever someone needs to perform anything on the object, it asks the TM to either do it on the requester's behalf or to assign the object to the requesting task and prevent anyone else from touching it until further notice.
When an operation involves multiple objects, they are simply all passed to the TM/CA.
In a transaction request, only concurrent enabled objects are allowed.
In case there are operations and side effects which belong to the code block inside the transaction but don't really need to be transactional, an escape block can be used.
The biggest difficulty is with function/procedure handling calls and I/O or syscalls. Exceptions and errors, too.
Irrevocable ops?

def procedure concurrent sqrt_wrapper (IN x number, IN y number, OUT z number, INOUT counter_attempts number)	//restricted polymorphism : only numerical types allowed
begin atomic
counter_attempts++
if x < y
	abort
else
	if z null
		abort noretry	"OUT parameter z is null" //No auto-retry of transaction - retrying won't fix z being unallocated
	else
		def tmp := x-y	//guaranteed to be >= 0
		z := sqrt( tmp )	//OK as tmp >= 0
	end if
end if
end atomic
end procedure concurrent

def count uint16	//default-init to zero<type> which is 0 for an int
def x := 0.0, y := 1.0, z := OUT	//z is explicitly left uninitialized in expectation of being written to 
	

TM/CA:
One global, per-program entity, possibly with Execution-domain-level, Thread-level, etc. assistants/subordinates.
Creation of transactional objects:

 transactional x := create TypeName([<<construction params>>]) [concurrent access policies]

 Concurrent access policies can be various...
 
 At the end of scope, refcount of the actual concurrent object is decremented.
 Module-level concurrent objects are also refcounted. Globals which actually live in the global module??
 When the refcount drops to 0, there can be no further uses => object is deleted.
 
 The "concurrent x" is a handle on the object. All and any access happens through that handle.
 A transaction is then a logically atomic operation which is executed under the TM/CA and BY the TM/CA, on behalf of the requesting task.
 A transaction is nothing more than a specially formulated and packaged task which is given to the TM/CA to execute.
 tr := [deferred] transaction { /*protected stuff*/ }
 Deferred transactions are launched on demand, possibly many times. A bit like stored procedures in a DBMS.
 Non-deferred transactions are launched immediately when constructed.
 When the compiler sees a transaction definition, it does:
 - Extract the list of mentioned non-immutable objects;
 - Peek into the mentioned functions and procedures (recursive scan). If they contain mentions of non-immutable transactional objects, they are added to the list.
 - Functions and procedures require no special transactional marking if they contain only allowed operations. Some kinds of operations are generally not permitted inside transactions, however : direct hardware access (ports, MMIO, ...), direct memory access, network IO, filesystem IO, queue accesses etc. The reason is that those are not (easily) revocable. The user is required to either use custom buffering schemes, or the specilized transaction-safe variants provided by the language.
 - If mutable non-transactional objects are mentioned, it is a compilation error. If the programmer does indeed want to use them inside a transaction, they must be marked in the text of the transaction block as non-transactional. Such objects are left entirely to the care of the programmer.
 - If the programmer wishes, an entire expression or statement, possibly including arbitrary calls, can be put inside an explicit non-transactional block.
 - Inside a transaction block, in a transaction context, the "_TRANSACTION_" "macro" is defined as true; in non-transactional escape blocks, it's set back to false.
 - Whenever in transaction context, a set of abort/retry current/... constructs are allowed to allow flexibility and expressivity.
 - Transactions, being special tasks, allow for partially concurrent execution. A transaction task can contain task-parallel and data-parallel constructs, with some limitations, of course.
 - Memory and other resource allocations/reservations made inside a transaction task are freed when an attempt fails, and reacquired on a next attempt (some resources may be cached). When a transaction task is exited for good, everything is deleted. Objects initialized are deleted, too, and so on. Thus, a transaction task acts like any other task or simple scope.
 - Transactions compose the way ordinary tasks compose.
 - What the TM/CA does :  it (re)orders/monitors/traces operations and ensures they are not interfering with one another. A very simple TM/CA will just grab transaction tasks off a single queue and execute them sequentially. A more sophisticated one will do finer stuff, like RCU, fine-grained locking, virtual machine interpretation, memory shadowing, shadowing of other resources, conflict detection, etc.

 
 
 

-- Support for region-based allocation. Named regions.
Regions are conceptually contiguous blocks of (virtual) memory, intended to permit quick allocation and release in one go. So, it's similar to both a buffer and a separate non-contended-for arena.
There are two kinds of regions : one for objects which require no destruction/finalization and the second for objects which (may) require that.
 def x:= new string(reserve:80) within rgn1
 ....
 delete region rgn1
 
 Stuff allocated within a region is NOT allowed to have references to anything outside the region; the region has a single owner - the region descriptor (a kind of unique ref), and all access must go through it. Taking direct references to objects inside it is forbidden and prevented. An object must be explicitly copied out of the region.
 When a region is deleted, destructors/finalizers are run, 
 

